[Skip to main content](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#main-content)

[![Google AI for Developers](https://www.gstatic.com/devrel-devsite/prod/ve761bca974e16662f27aa8810df6d144acde5bdbeeca0dfd50e25f86621eaa19/googledevai/images/lockup-new.svg)](https://ai.google.dev/)

`/`

- [English](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js)
- [Deutsch](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=de)
- [Español – América Latina](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=es-419)
- [Français](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=fr)
- [Indonesia](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=id)
- [Italiano](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=it)
- [Polski](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=pl)
- [Português – Brasil](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=pt-br)
- [Shqip](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=sq)
- [Tiếng Việt](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=vi)
- [Türkçe](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=tr)
- [Русский](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=ru)
- [עברית](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=he)
- [العربيّة](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=ar)
- [فارسی](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=fa)
- [हिंदी](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=hi)
- [বাংলা](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=bn)
- [ภาษาไทย](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=th)
- [中文 – 简体](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=zh-cn)
- [中文 – 繁體](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=zh-tw)
- [日本語](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=ja)
- [한국어](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=ko)

[Sign in](https://ai.google.dev/_d/signin?continue=https%3A%2F%2Fai.google.dev%2Fedge%2Fmediapipe%2Fsolutions%2Fgenai%2Fllm_inference%2Fweb_js&prompt=select_account)

- On this page
- [Quickstart](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#quickstart)
  - [Add dependencies](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#add-dependencies)
  - [Download a model](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#download-model)
  - [Initialize the Task](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#initialize-task)
  - [Run the Task](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#run-task)
- [Sample application](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#sample-application)
- [Configuration options](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#configuration-options)
- [Model conversion](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#model-conversion)
- [LoRA customization](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#lora-customization)
  - [Prepare LoRA weights](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#prepare-lora)
  - [Model conversion](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#lora-model-conversion)
  - [LoRA model inference](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#lora-model)

**Introducing LiteRT**: Google's high-performance runtime for on-device AI, formerly known as TensorFlow Lite. [Learn more](https://developers.googleblog.com/en/tensorflow-lite-is-now-litert)

- [Home](https://ai.google.dev/)
- [Google AI Edge](https://ai.google.dev/edge)
- [Solutions](https://ai.google.dev/edge/mediapipe/solutions/guide)

Was this helpful?



 Send feedback



# LLM Inference guide for Web

- On this page
- [Quickstart](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#quickstart)
  - [Add dependencies](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#add-dependencies)
  - [Download a model](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#download-model)
  - [Initialize the Task](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#initialize-task)
  - [Run the Task](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#run-task)
- [Sample application](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#sample-application)
- [Configuration options](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#configuration-options)
- [Model conversion](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#model-conversion)
- [LoRA customization](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#lora-customization)
  - [Prepare LoRA weights](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#prepare-lora)
  - [Model conversion](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#lora-model-conversion)
  - [LoRA model inference](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#lora-model)

The LLM Inference API lets you run large language models (LLMs) completely on-device
for Web applications, which you can use to perform a wide range of tasks, such
as generating text, retrieving information in natural language form, and
summarizing documents. The task provides built-in support for multiple
text-to-text large language models, so you can apply the latest on-device
generative AI models to your Web apps.

To quickly add the LLM Inference API to your Web application, follow the
[Quickstart](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#quickstart). For a basic example of a Web application running the
LLM Inference API, see the [sample application](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#sample-application). For a more
in-depth understanding of how the LLM Inference API works, refer to the
[configuration options](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#configuration-options), [model\\
conversion](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#model-conversion), and [LoRA tuning](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js#lora-customization) sections.

You can see this task in action with the [MediaPipe Studio\\
demo](https://mediapipe-studio.webapps.google.com/studio/demo/llm_inference).
For more information about the capabilities, models, and configuration options
of this task, see the [Overview](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/index).

## Quickstart

Use the following steps to add the LLM Inference API to your Web application. The
LLM Inference API requires a web browser with WebGPU compatibility. For a full list
of compatible browsers, see [GPU browser\\
compatibility](https://developer.mozilla.org/en-US/docs/Web/API/GPU#browser_compatibility).

### Add dependencies

The LLM Inference API uses the
[`@mediapipe/tasks-genai`](https://www.npmjs.com/package/@mediapipe/tasks-genai)
package.

Install the required packages for local staging:

```
npm install @mediapipe/tasks-genai

```

To deploy to a server, use a content delivery network (CDN) service like
[jsDelivr](https://www.jsdelivr.com/) to add code directly to your HTML page:

```
<head>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai/genai_bundle.cjs"
    crossorigin="anonymous"></script>
</head>

```

### Download a model

Download Gemma-2 2B in an 8-bit quantized format from [Kaggle\\
Models](https://www.kaggle.com/models/google/gemma-2/tfLite/gemma2-2b-it-gpu-int8).
For more information on the available models, see the [Models\\
documentation](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/index#models).

Store the model within your project directory:

```
<dev-project-root>/assets/gemma-2b-it-gpu-int8.bin

```

Specify the path of the model with the `baseOptions` object `modelAssetPath`
parameter:

```
baseOptions: { modelAssetPath: `/assets/gemma-2b-it-gpu-int8.bin`}

```

### Initialize the Task

Initialize the task with basic configuration options:

```
const genai = await FilesetResolver.forGenAiTasks(
    // path/to/wasm/root
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai@latest/wasm"
);
llmInference = await LlmInference.createFromOptions(genai, {
    baseOptions: {
        modelAssetPath: '/assets/gemma-2b-it-gpu-int8.bin'
    },
    maxTokens: 1000,
    topK: 40,
    temperature: 0.8,
    randomSeed: 101
});

```

### Run the Task

Use the `generateResponse()` function to trigger inferences.

```
const response = await llmInference.generateResponse(inputPrompt);
document.getElementById('output').textContent = response;

```

To stream the response, use the following:

```
llmInference.generateResponse(
  inputPrompt,
  (partialResult, done) => {
        document.getElementById('output').textContent += partialResult;
});

```

## Sample application

The sample application is an example of a basic text generation app for Web,
using the LLM Inference API. You can use the app as a starting point for your own
Web app, or refer to it when modifying an existing app. The example code is
hosted on
[GitHub](https://github.com/google-ai-edge/mediapipe-samples/tree/main/examples/llm_inference/js).

Clone the git repository using the following command:

```
git clone https://github.com/google-ai-edge/mediapipe-samples

```

For more information, see the [Setup Guide for\\
Web](https://ai.google.dev/mediapipe/solutions/setup_web).

## Configuration options

Use the following configuration options to set up a Web app:

| Option Name | Description | Value Range | Default Value |
| --- | --- | --- | --- |
| `modelPath` | The path to where the model is stored within the project directory. | PATH | N/A |
| `maxTokens` | The maximum number of tokens (input tokens + output tokens) the model handles. | Integer | 512 |
| `topK` | The number of tokens the model considers at each step of generation.<br> Limits predictions to the top k most-probable tokens. | Integer | 40 |
| `temperature` | The amount of randomness introduced during generation. A higher<br> temperature results in more creativity in the generated text, while a<br> lower temperature produces more predictable generation. | Float | 0.8 |
| `randomSeed` | The random seed used during text generation. | Integer | 0 |
| `loraRanks` | LoRA ranks to be used by the LoRA models during runtime. Note: this is only compatible with GPU models. | Integer array | N/A |

## Model conversion

The LLM Inference API is compatible with the following types of models, some of
which require model conversion. Use the table to identify the required steps
method for your model.

| Models | Conversion method | Compatible platforms | File type |
| --- | --- | --- | --- |
| Gemma-3 1B | No conversion required | Android, web | .task |
| Gemma 2B, Gemma 7B, Gemma-2 2B | No conversion required | Android, iOS, web | .bin |
| Phi-2, StableLM, Falcon | [MediaPipe conversion script](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/index#supported-models) | Android, iOS, web | .bin |
| All PyTorch LLM models | [AI Edge Torch Generative library](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/index#pytorch-models) | Android, iOS | .task |

To learn how you can convert other models, see the [Model\\
Conversion](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/index#convert-model) section.

## LoRA customization

The LLM Inference API supports LoRA (Low-Rank Adaptation) tuning using the
[PEFT](https://huggingface.co/docs/peft/main/en/index) (Parameter-Efficient
Fine-Tuning) library. LoRA tuning customizes the behavior of LLMs through a
cost-effective training process, creating a small set of trainable weights based
on new training data rather than retraining the entire model.

The LLM Inference API supports adding LoRA weights to attention layers of the
[Gemma-2 2B](https://huggingface.co/google/gemma-2-2b), [Gemma\\
2B](https://huggingface.co/google/gemma-2b) and
[Phi-2](https://huggingface.co/microsoft/phi-2) models. Download the model in
the `safetensors` format.

The base model must be in the `safetensors` format in order to create LoRA
weights. After LoRA training, you can convert the models into the FlatBuffers
format to run on MediaPipe.

### Prepare LoRA weights

Use the [LoRA\\
Methods](https://huggingface.co/docs/peft/main/en/task_guides/lora_based_methods)
guide from PEFT to train a fine-tuned LoRA model on your own dataset.

The LLM Inference API only supports LoRA on attention layers, so only specify the
attention layers in `LoraConfig`:

```
# For Gemma
from peft import LoraConfig
config = LoraConfig(
    r=LORA_RANK,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
)

# For Phi-2
config = LoraConfig(
    r=LORA_RANK,
    target_modules=["q_proj", "v_proj", "k_proj", "dense"],
)

```

After training on the prepared dataset and saving the model, the fine-tuned LoRA
model weights are available in `adapter_model.safetensors`. The `safetensors`
file is the LoRA checkpoint used during model conversion.

### Model conversion

Use the MediaPipe Python Package to convert the model weights into the
Flatbuffer format. The `ConversionConfig` specifies the base model options along
with the additional LoRA options.

```
import mediapipe as mp
from mediapipe.tasks.python.genai import converter

config = converter.ConversionConfig(
  # Other params related to base model
  ...
  # Must use gpu backend for LoRA conversion
  backend='gpu',
  # LoRA related params
  lora_ckpt=LORA_CKPT,
  lora_rank=LORA_RANK,
  lora_output_tflite_file=LORA_OUTPUT_FILE,
)

converter.convert_checkpoint(config)

```

The converter will produce two MediaPipe-compatible files, one for the base
model and another for the LoRA model.

### LoRA model inference

Web supports dynamic LoRA during runtime, meaning users declare the LoRA ranks
during initialization. This means you can swap out different LoRA models during
runtime.

```
const genai = await FilesetResolver.forGenAiTasks(
    // path/to/wasm/root
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai@latest/wasm"
);
const llmInference = await LlmInference.createFromOptions(genai, {
    // options for the base model
    ...
    // LoRA ranks to be used by the LoRA models during runtime
    loraRanks: [4, 8, 16]
});

```

Load the LoRA models during runtime, after initializing the base model. Trigger
the LoRA model by passing the model reference when generating the LLM response.

```
// Load several LoRA models. The returned LoRA model reference is used to specify
// which LoRA model to be used for inference.
loraModelRank4 = await llmInference.loadLoraModel(loraModelRank4Url);
loraModelRank8 = await llmInference.loadLoraModel(loraModelRank8Url);

// Specify LoRA model to be used during inference
llmInference.generateResponse(
  inputPrompt,
  loraModelRank4,
  (partialResult, done) => {
        document.getElementById('output').textContent += partialResult;
});

```

Was this helpful?



 Send feedback



Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-04-01 UTC.