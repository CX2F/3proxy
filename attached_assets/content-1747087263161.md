[Skip to content](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_mediapipe_image_generation.ipynb#start-of-content)

[GoogleCloudPlatform](https://github.com/GoogleCloudPlatform)/ **[vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples)** Public

- [Notifications](https://github.com/login?return_to=%2FGoogleCloudPlatform%2Fvertex-ai-samples) You must be signed in to change notification settings
- [Fork\\
117](https://github.com/login?return_to=%2FGoogleCloudPlatform%2Fvertex-ai-samples)
- [Star\\
338](https://github.com/login?return_to=%2FGoogleCloudPlatform%2Fvertex-ai-samples)


## Files

main

/

# model\_garden\_mediapipe\_image\_generation.ipynb

Copy path

Blame

Blame

## Latest commit

![dstnluong-google](https://avatars.githubusercontent.com/u/129889805?v=4&size=40)![copybara-github](https://avatars.githubusercontent.com/u/32201506?v=4&size=40)

[dstnluong-google](https://github.com/GoogleCloudPlatform/vertex-ai-samples/commits?author=dstnluong-google)

and

[copybara-github](https://github.com/GoogleCloudPlatform/vertex-ai-samples/commits?author=copybara-github)

[Set system\_labels in notebooks](https://github.com/GoogleCloudPlatform/vertex-ai-samples/commit/883e1e5fc12ff3ec9476737a861677926efa44e1)

Jan 7, 2025

[883e1e5](https://github.com/GoogleCloudPlatform/vertex-ai-samples/commit/883e1e5fc12ff3ec9476737a861677926efa44e1) · Jan 7, 2025

## History

[History](https://github.com/GoogleCloudPlatform/vertex-ai-samples/commits/main/notebooks/community/model_garden/model_garden_mediapipe_image_generation.ipynb)

1006 lines (1006 loc) · 36 KB

·

/

# model\_garden\_mediapipe\_image\_generation.ipynb

Top

## File metadata and controls

- Preview

- Code

- Blame


1006 lines (1006 loc) · 36 KB

·

[Raw](https://github.com/GoogleCloudPlatform/vertex-ai-samples/raw/refs/heads/main/notebooks/community/model_garden/model_garden_mediapipe_image_generation.ipynb)

Loading

Notebooks

In \[ \]:

\# Copyright 2023 Google LLC#\# Licensed under the Apache License, Version 2.0 (the "License");\# you may not use this file except in compliance with the License.\# You may obtain a copy of the License at#\# https://www.apache.org/licenses/LICENSE-2.0#\# Unless required by applicable law or agreed to in writing, software\# distributed under the License is distributed on an "AS IS" BASIS,\# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\# See the License for the specific language governing permissions and\# limitations under the License.

# Vertex AI Model Garden - Image Generation with MediaPipe [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Vertex-AI-Model-Garden---Image-Generation-with-MediaPipe)

|     |     |     |
| --- | --- | --- |
| [![Colab logo](https://camo.githubusercontent.com/d1ede918a0816f426c3669677756c6ccadc333b376d71b4374d429ecf07bcab9/68747470733a2f2f636c6f75642e676f6f676c652e636f6d2f6d6c2d656e67696e652f696d616765732f636f6c61622d6c6f676f2d333270782e706e67) Run in Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_mediapipe_image_generation.ipynb) | [![GitHub logo](https://camo.githubusercontent.com/d22e6f28ce07f51a5bcc74bbd8d4a0dac818fdca9d7656d4c0afa1d525591cbb/68747470733a2f2f636c6f75642e676f6f676c652e636f6d2f6d6c2d656e67696e652f696d616765732f6769746875622d6c6f676f2d333270782e706e67)\<br> View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_mediapipe_image_generation.ipynb) | [![Vertex AI logo](https://camo.githubusercontent.com/8e904d6425a2c88b25e4bf09841cd3a36972eb61d77d5fc278f24967fe5c7d14/68747470733a2f2f6c68332e676f6f676c6575736572636f6e74656e742e636f6d2f55694e6f6f59344c5567575f6f547670734e685070517a7373745635573846377259677867474244383563574a6f4c6d724f7a6856735f6b734b5f7667783430534873376a43716b546b436b3d6531342d726a2d736330786666666666662d683133302d773332)\<br>Open in Vertex AI Workbench](https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_mediapipe_image_generation.ipynb) |

**_NOTE_**: This notebook has been tested in the following environment:

- Python version = 3.9

**NOTE**: The checkpoint and the dataset linked in this Colab are not owned or distributed by Google, and are made available by third parties. Please review the terms and conditions made available by the third parties before using the checkpoint and data.

## Overview [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Overview)

This notebook demonstrates how you can customize a [MediaPipe Image Generator](https://developers.google.com/mediapipe/solutions/vision/image_generator), a text-to-image generator, by adding Low-Rank Adaptation ( [LoRA](https://arxiv.org/abs/2106.09685)) weights to generate images of specific people, objects, and styles.

Using Vertex AI's Model Garden, we will retrain a standard diffusion model on specialized dataset of specific concepts, which are identified by unique tokens. With the new LoRA weights after training, the new model is able to generate images of the new concept when the token is specified in the text prompt.

Once the model is customized with LoRA weights, it should only be used to generate images of the tokenized concept. It is no longer useful as a generalized image generation model. For more on customizing a MediaPipe Image Generator with LoRA weights, see the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/image_generator#lora).

NOTE: If you are creating LoRa weights to generate images of specific people and faces, only use this solution on your face or faces of people who have given you permission to do so.

### Objective [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Objective)

- Set up a Google Cloud project with Vertex AI.
- Train a text-to-image difussion model on a specialized dataset to create [LoRA](https://arxiv.org/abs/2106.09685) weights.
- Customize a general image generator into a specialized generator that can inject specific objects, people, and styles into generated images.
- Configure the newly trained Image Generator.
- Download, upload, and deploy the new model

### Costs [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Costs)

This tutorial uses billable components of Google Cloud:

- Vertex AI
- Cloud Storage

Learn about [Vertex AI\\
pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\\
pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\\
Calculator](https://cloud.google.com/products/calculator/)
to generate a cost estimate based on your projected usage.

## Before you begin [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Before-you-begin)

### Set up your Google Cloud project [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Set-up-your-Google-Cloud-project)

**The following steps are required, regardless of your notebook environment.**

1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).

2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).

3. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).

4. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).

5. Enter your project ID in the cell below. Then run the cell to make sure the
Cloud SDK uses the right project for all the commands in this notebook.


### Authenticate for Colab [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Authenticate-for-Colab)

**Note**: Skip this step if you are not using [Colab](https://colab.google/)

Run the following commands to install dependencies and authenticate with Google Cloud on Colab.

In \[ \]:

! pip3 install --upgrade pip

import sys

if"google.colab"in sys.modules:
! pip3 install --upgrade google-cloud-aiplatform

\# Automatically restart kernel after installsimport IPython

app = IPython.Application.instance()
app.kernel.do\_shutdown(True)

from google.colab import auth as google\_auth

google\_auth.authenticate\_user()


### Set your project ID ( `PROJECT_ID`) [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Set-your-project-ID-(PROJECT_ID))

If you don't know your project ID, try the following:

- Run `gcloud config list`.
- Run `gcloud projects list`.
- See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)

In \[ \]:

PROJECT\_ID = ""\# @param {type:"string"}\# Set the project id
! gcloud config set project {PROJECT\_ID}


### Set the storage location ( `REGION`) [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Set-the-storage-location-(REGION))

You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations).

In \[ \]:

REGION = ""\# @param {type: "string"}
REGION\_PREFIX = REGION.split("-")\[0\]
assert REGION\_PREFIX in (
"us",
"europe",
"asia",
), f'{REGION} is not supported. It must be prefixed by "us", "asia", or "europe".'

### Create a Cloud Storage bucket [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Create-a-Cloud-Storage-bucket)

Create a storage bucket to store intermediate artifacts such as datasets and trained models.

In \[ \]:

BUCKET\_URI = ""\# @param {type:"string"}

If your bucket doesn't already exist, create your Cloud Storage bucket.

**NOTE**: Only run the following cell if you do not already have a bucket.

In \[ \]:

! gsutil mb -l {REGION} -p {PROJECT\_ID} {BUCKET\_URI}


### Import libraries [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Import-libraries)

In \[ \]:

import json
import os
from datetime import datetime

from google.cloud import aiplatform


### Initialize Vertex AI SDK for Python [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Initialize-Vertex-AI-SDK-for-Python)

Initialize the Vertex AI SDK for Python for your project.

In \[ \]:

now = datetime.now().strftime("%Y%m%d-%H%M%S")

STAGING\_BUCKET = os.path.join(BUCKET\_URI, "temp/%s" % now)

MODEL\_EXPORT\_PATH = os.path.join(STAGING\_BUCKET, "model")

IMAGE\_EXPORT\_PATH = os.path.join(STAGING\_BUCKET, "image")

aiplatform.init(project=PROJECT\_ID, location=REGION, staging\_bucket=STAGING\_BUCKET)


### Define training and serving constants [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Define-training-and-serving-constants)

In \[ \]:

TRAINING\_JOB\_DISPLAY\_NAME = "mediapipe\_stable\_diffusion\_%s" % now
TRAINING\_CONTAINER = f"{REGION\_PREFIX}-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/mediapipe-stable-diffusion-train"
TRAINING\_MACHINE\_TYPE = "a2-highgpu-1g"
TRAINING\_ACCELERATOR\_TYPE = "NVIDIA\_TESLA\_A100"
TRAINING\_ACCELERATOR\_COUNT = 1

PREDICTION\_CONTAINER\_URI = f"{REGION\_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240403\_0836\_RC00"
PREDICTION\_PORT = 7080
PREDICTION\_ACCELERATOR\_TYPE = "NVIDIA\_TESLA\_V100"
PREDICTION\_MACHINE\_TYPE = "n1-standard-8"
UPLOAD\_MODEL\_NAME = "mediapipe\_stable\_diffusion\_model\_%s" % now


## Train a customized Image Generator [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Train-a-customized-Image-Generator)

In this section, we will customize the Image Generator by training the model on images of [teapots](https://github.com/google/dreambooth/tree/main/dataset/teapot) from the [DreamBooth dataset](https://github.com/google/dreambooth/tree/main). Using the LoRA weights created through training, the new model will be able to inject teapots into generated images.

This is a simple example implementation. You can modify the following cells to further customize the notebook.

### Choose the pre-trained model to download [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Choose-the-pre-trained-model-to-download)

The MediaPipe Image Generator task requires you to download a trained model that matches the `runwayml/stable-diffusion-v1-5 EMA-only` model format, based on the following model: [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/).

In \[ \]:

unet\_url = "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/unet/diffusion\_pytorch\_model.bin"\# @param {type:"string"}
vae\_url = "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/vae/diffusion\_pytorch\_model.bin"\# @param {type:"string"}
text\_encoder\_url = "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/text\_encoder/pytorch\_model.bin"\# @param {type:"string"}

### Prepare input data for training [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Prepare-input-data-for-training)

Customizing a model for image generation requires a dataset that contains sample pictures of the concept instance that you want to use in the generation. The concept can be a person, object, or style.

**Object**![](https://camo.githubusercontent.com/0c944e7014d7ceeade114b5e9464bca58901a34764ca4c457f97e98fd5a8d5d4/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f6d65646961706970652d6173736574732f646f63756d656e746174696f6e2f6f626a6563745f6c6f72612e706e67)

**Person**![](https://camo.githubusercontent.com/39d210679a7e91a946bb09e4795c3c2899bef02b1a33b3f2119583f8e135ecda/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f6d65646961706970652d6173736574732f646f63756d656e746174696f6e2f706572736f6e5f6c6f72612e706e67)

**Style**![](https://camo.githubusercontent.com/76ed316052a37f382b2735ebac0ead67d8d2ef12ab76063121e0c8bd894a5a43/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f6d65646961706970652d6173736574732f646f63756d656e746174696f6e2f7374796c655f6c6f72612e706e67)

You must also assign a unique token to the new concept. The prompt should include the token, which is "monadikos" in this case, followed by a word that describes the concept to generate. In this example, we are using "A monadikos teapot". The images from the [teapots](https://github.com/google/dreambooth/tree/main/dataset/teapot) dataset can be downloaded from Google Cloud Storage.

The customized model will recognize the term "monadikos teapot", and inject an image of a teapot into the generated images.

In \[ \]:

\# Path to the training data folder.
training\_data\_path = "gs://mediapipe-tasks/image\_generator/teapot"\# @param {type:"string"}\# An instance description of the training data.
training\_data\_prompt = "A monadikos teapot"\# @param {type:"string"}

### Set training options [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Set-training-options)

The Image Generator comes with a set of pre-defined hyperparameter ( `HParams`) settings that work best for specific situations. You should select a template that best matches your use case.

You can further customize hyperparameters like the learning rate and the number of training steps (epochs). For more information on these hyperparameters, see the [Google Machine Learning glossary](https://developers.google.com/machine-learning/glossary)

To set custom training parameters, adjust the values for the following hyperparameters:

In \[ \]:

\# Parameters about training configuration\# The learning rate to use for gradient descent training.
learning\_rate: float = 0.00001\# @param {type:"number"}\# Number of training steps. If set to 0, uses the default value.
num\_train\_steps: int = 0\# @param {type:"integer"}\# Save the checkpoint in every n steps.
save\_checkpoints\_every\_n: int = 100\# @param {type:"integer"}\# Batch size for training.
batch\_size: int = 1\# @param {type:"integer"}\# Dataset-related parameters\# Whether to use random horizontal flip on data.
random\_flip: bool = False\# @param {type:"boolean"}\# Whether to use random largest square crop.
random\_crop: bool = False\# @param {type:"boolean"}\# Whether to distort the color of the image (jittering order is random).
random\_color\_jitter: bool = False\# @param {type:"boolean"}\# Hyperparameters for LoRA tuning\# The rank in the low-rank matrices. If set to 0, uses the default value.
lora\_rank: int = 0\# @param {type:"integer"}

Alternatively, you can also use one of our pre-trained models for these templates. These templates are already customized and already contain LoRA weights:

- [Object (berry bowls)](https://storage.googleapis.com/mediapipe-tasks/image_generator/object/pytorch_lora_weights.bin)
- [Face](https://storage.googleapis.com/mediapipe-tasks/image_generator/face/pytorch_lora_weights.bin)
- [Style](https://storage.googleapis.com/mediapipe-tasks/image_generator/style/pytorch_lora_weights.bin)

In \[ \]:

template = ""\# @param \["", "face", "object", "style"\]

## Test the customized Image Generator model [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Test-the-customized-Image-Generator-model)

After training the custom model, we will generate images to examine the quality of the customized model. You can provide a text prompt below and configure options for generating the test images.

### Define the test generation prompt [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Define-the-test-generation-prompt)

Specify the prompt to use to test the customized model. Note that a variation of the token, "monadikos teapots", is included in the prompt. If you are customizing this notebook with another dataset, set a token to describe the object, person, or style depicted in the training data.

In \[ \]:

prompt: str = "Two monadikos teapots on a table"\# @param {type:"string"}

### Configure the parameters to generate test images [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Configure-the-parameters-to-generate-test-images)

Set configuration options to run image generation with the customized model.

In \[ \]:

\# Number of steps to run inference.
number\_inference\_steps: int = 50\# @param {type:"integer"}\# Classifier-free guidance weight to use during inference. Weight must be is >= 1.0.
guidance\_scale: float = 7.5\# @param {type:"number"}\# Number of generated images per prompt.
number\_generated\_images: int = 8\# @param {type:"integer"}

### Tune the image generator with LoRA [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Tune-the-image-generator-with-LoRA)

Tune the Image Generator with LoRA and generate new images based on your prompt. This can take up to 10 minutes on Vertex AI with a A100 GPU.

In \[ \]:

model\_export\_path = MODEL\_EXPORT\_PATH
image\_export\_path = IMAGE\_EXPORT\_PATH

worker\_pool\_specs = \[\
{\
"machine\_spec": {\
"machine\_type": TRAINING\_MACHINE\_TYPE,\
"accelerator\_type": TRAINING\_ACCELERATOR\_TYPE,\
"accelerator\_count": TRAINING\_ACCELERATOR\_COUNT,\
},\
"replica\_count": 1,\
"container\_spec": {\
"image\_uri": TRAINING\_CONTAINER,\
"command": \[\],\
"args": \[\
"--task\_name=stable\_diffusion",\
"--model\_export\_path=%s" % model\_export\_path,\
"--image\_export\_path=%s" % image\_export\_path,\
"--training\_data\_path=%s" % training\_data\_path,\
"--training\_data\_prompt='%s'" % training\_data\_prompt,\
"--prompt='%s'" % prompt,\
"--hparams\_template=%s" % template,\
"--hparams=%s"\
% json.dumps(\
{\
"learning\_rate": learning\_rate,\
"num\_train\_steps": num\_train\_steps,\
"save\_checkpoints\_every\_n": save\_checkpoints\_every\_n,\
"batch\_size": batch\_size,\
"random\_flip": random\_flip,\
"random\_crop": random\_crop,\
"random\_color\_jitter": random\_color\_jitter,\
"lora\_rank": lora\_rank,\
"torch\_vae": vae\_url,\
"torch\_unet": unet\_url,\
"torch\_text\_encoder": text\_encoder\_url,\
}\
),\
"--generator\_hparams=%s"\
% json.dumps(\
{\
"number\_inference\_steps": number\_inference\_steps,\
"guidance\_scale": guidance\_scale,\
"number\_generated\_images": number\_generated\_images,\
}\
),\
\],\
},\
}\
\]

training\_job = aiplatform.CustomJob(
display\_name=TRAINING\_JOB\_DISPLAY\_NAME,
project=PROJECT\_ID,
worker\_pool\_specs=worker\_pool\_specs,
staging\_bucket=STAGING\_BUCKET,
)

training\_job.run()


## Download images and model [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Download-images-and-model)

After training and testing the new model, you can download the generated images and the new customized model. The LoRA weights from training can also be used with the MediaPipe Tasks ImageGenerator API for on-device applications.

### Download generated images [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Download-generated-images)

Download and preview the generated images at different checkpoints.
Inspecting the generated images helps to determine the best checkpoint and avoid underfitting or overfitting.

In \[ \]:

import sys

import matplotlib.pyplot as plt

defcopy\_image(images\_source, images\_dest):
os.makedirs(images\_dest, exist\_ok=True)
! gsutil cp -r {images\_source}/\* {images\_dest}

local\_image\_path = "./images/"
copy\_image(IMAGE\_EXPORT\_PATH, local\_image\_path)

steps\_samples = {}
for filename in os.listdir(local\_image\_path):
absolute\_path = os.path.join(local\_image\_path, filename)
if os.path.isfile(absolute\_path):
parsed\_name = filename.split("\_")
step = int(parsed\_name\[1\])
if step notin steps\_samples:
steps\_samples\[step\] = \[\]
image = plt.imread(absolute\_path)
steps\_samples\[step\].append(image)

for step insorted(steps\_samples.keys()):
print(f"\\nGenerated image with training steps {step}:")
for image in steps\_samples\[step\]:
plt.figure(figsize=(20, 10), dpi=150)
plt.axis("off")
plt.imshow(image)
plt.show()


By default, the last checkpoint is used for deployment. However, we can customize that here based on the above visual inspection.

In \[ \]:

deployed\_checkpoint: int = -1\# @param {type:"integer"}if deployed\_checkpoint == -1:
deployed\_checkpoint = num\_train\_steps
valid\_checkpoints = list(
range(save\_checkpoints\_every\_n, num\_train\_steps + 1, save\_checkpoints\_every\_n)
)
if deployed\_checkpoint notin valid\_checkpoints:
raise ValueError("Invalid checkpoint chosen for deployment.")


### Download model [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Download-model)

After fine-tuning and evaluating the model, you can download the model and checkpoints.

In \[ \]:

import sys

defcopy\_model(model\_source, model\_dest):
os.makedirs(model\_dest, exist\_ok=True)
! gsutil -m cp -r {model\_source}/\* {model\_dest}

local\_model\_path = "/models"
copy\_model(MODEL\_EXPORT\_PATH, local\_model\_path)

! tar czf models.tar.gz {local\_model\_path}/\*

if"google.colab"in sys.modules:
from google.colab import files

files.download("models.tar.gz")


## Upload and deploy to Vertex AI [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Upload-and-deploy-to-Vertex-AI)

This section shows the way to test with trained models.

1. Upload and deploy models to the [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)
2. Get [online predictions](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions) from the deployed model

### Upload model to Vertex AI Model Registry [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Upload-model-to-Vertex-AI-Model-Registry)

In \[ \]:

lora\_id = os.path.join(MODEL\_EXPORT\_PATH, f"checkpoint\_{deployed\_checkpoint}")

serving\_env = {
"TASK": "text-to-image",
"MODEL\_ID": "runwayml/stable-diffusion-v1-5",
"LORA\_ID": lora\_id,
"DEPLOY\_SOURCE": "notebook",
}

model = aiplatform.Model.upload(
display\_name=UPLOAD\_MODEL\_NAME,
serving\_container\_image\_uri=PREDICTION\_CONTAINER\_URI,
serving\_container\_ports=\[PREDICTION\_PORT\],
serving\_container\_predict\_route="/predictions/diffusers\_serving",
serving\_container\_health\_route="/ping",
serving\_container\_environment\_variables=serving\_env,
)

model.wait()

print("The uploaded model name is: ", UPLOAD\_MODEL\_NAME)


### Deploy the uploaded model [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Deploy-the-uploaded-model)

You will deploy models in Google Cloud Vertex AI. The default setting will use 1 V100 GPU for deployment.

Please create a Service Account for serving with dockers if you do not have one yet.

The model deployment will take around 1 minute to finish.

In \[ \]:

\# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\# and create service account with \`Vertex AI User\` and \`Storage Object Admin\` roles.
service\_account = ""\# @param {type:"string"}

endpoint = aiplatform.Endpoint.create(display\_name=f"{UPLOAD\_MODEL\_NAME}-endpoint")
model.deploy(
endpoint=endpoint,
machine\_type=PREDICTION\_MACHINE\_TYPE,
accelerator\_type=PREDICTION\_ACCELERATOR\_TYPE,
accelerator\_count=1,
deploy\_request\_timeout=1800,
service\_account=service\_account,
system\_labels={
"NOTEBOOK\_NAME": "model\_garden\_mediapipe\_image\_generation.ipynb"
},
)


The docker container still needs to download and load the model after the endpoint is created. Therefore, we recommend waiting for 3 extra minutes before proceeding to the next cell.

Once deployed, you can send a batch of text prompts to the endpoint to generate images.

In \[ \]:

import base64
from io import BytesIO

import matplotlib.pyplot as plt
from PIL import Image

instances = \[\
{"prompt": "Two monadikos teapots on a table"},\
{"prompt": "Two monadikos teapots on the floor"},\
\]
response = endpoint.predict(instances=instances)

plt.figure()
\_, grid = plt.subplots(1, len(instances))
for cell, prediction inzip(grid, response.predictions):
image = Image.open(BytesIO(base64.b64decode(prediction)))
cell.imshow(image)


## Clean up [¶](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/434e8ac8fc3b05994ea92644d8b09d1d1a25266b/notebooks/community/model_garden/\#Clean-up)

After the export is complete, you can delete your training job.

In \[ \]:

if training\_job.list(filter=f'display\_name="{TRAINING\_JOB\_DISPLAY\_NAME}"'):
training\_job.delete()
\# Undeploys models and deletes endpoints.
endpoint.delete(force=True)
model.delete()


You can also remove the output data.

In \[ \]:

!gsutil rm -r {STAGING\_BUCKET}